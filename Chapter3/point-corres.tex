\section{Predicting point correspondences}

Before delving into methods for 3D reconstruction, it is first necessary to discuss techniques for identifying \emph{point correspondences}. Point correspondences have a long history in computer vision for associating the same real-world location as it is represented by multiple camera views or on a 3D model surface. In a multi-image scenario, determining reliable correspondences between image pairs can be used to greatly reduce the ambiguity when reconstructing 3D scenes from 2D images. Even with only a single image available, correspondences can be predicted between the image and a representative 3D template mesh. This 3D-to-2D correspondence type is important for constraining the class of model fitting algorithms (discussed in depth later) which operate by aligning a 3D template mesh to a given 2D image. Of course, determining point correspondences is made more difficult in the presence of particular nuisance factors. In the case of animal imagery, we must associate points on a non-rigid object with independently moving parts (articulated), deal with frequent self-occlusion in which limbs overlap each other from the perspective of the camera, occlusion caused by environmental factors (e.g. trees, fences, humans etc.), varied and unknown backgrounds and a range of complex lighting conditions (including shadows). Throughout this section, the methods highlighted will be appraised against their suitability in this complex setting.

% https://link.springer.com/article/10.1007/s13735-019-00183-w
% https://arxiv.org/pdf/1603.09114.pdf

\subsection{Relating separate views of the same object/scene}

The first class of techniques focuses on classical approaches for determining corresponding image points taken of precisely the same (and almost always rigid) object. Early techniques focused on stereo~\cite{corres-stereo} or optical flow~\cite{corres-optflow} imagery, and matched image points based on finding regions with similar pixel intensities. Due to the adverse effects caused by changeable environmental factors (e.g. lighting) would have on the appearance of the real-world location when captured in separate images, attention moved towards designing schemes with improved robustness. Improvements were achieved when matching points based on local \emph{mid-level features} such as edges and corners, which have greater invariance to colour changes caused by lighting effects. Typical pipeliens would first identify \emph{interest points} (typically corners~\cite{corner-moravec,corner-harris,corner-susan} or blobs~\cite{sift}), from which local image patches could be compared according to either Squared Sum of Intensity Differences (SSD) or a cross-correlation (CC) scheme. Steady improvements were then made through the design of ever-improving feature descriptors, which encode local image information around points and aim for invariance against common transformations (e.g. viewpoint, rotation and scaling). Progress in this field arguably reached maturity with the advent of SIFT~\cite{sift}, which encodes points according to local histograms of graident orientations and was later speed-up by SURF~\cite{surf} and DAISY~\cite{daisy}. There have been modern attempts to learn sophisticated feature representations using convolutional neural network architectures~\cite{lift, matchnet}, which are shown to offer still further improvement.

The primary aim of these systems is to derive point correspondences between multiple views of the same object, usually as depicted in stereo images or between successive frames of a video. Unfortunately, by matching points based on local geometric features learnt from few image examples, these techniques do not readily extend to identifying correspondences between different instances of the same category. For example, matching SIFT features is likely to result in poor quality correspondences if tested on two dogs of different breeds due to the differing appearance and body geometry. For similar reasons, this class of techniques tend to deteriorate when tested on articulated objects since the object's structure can change and cause self-occlusion between views. The techniques are also known to suffer in scenarios with significant viewpoint changes (e.g. image of the front/back of an animal), since there are few correpsonding points available for matching. Finally, these techniques do not directly offer a method for identifying correspondences between an image and a representative 3D mesh. Although some work exists that extends some of the aforementioned feature descriptors (e.g. 3D-SIFT~\cite{sift-3d}) to 3D, matching typically requires a photorealistic 3D scan of the 2D subject which we cannot assume as input for our problem.

\subsection{Predicting keypoints with a semantic meaning}

This section will explore an alternative class of methods for identifying point correspondences. So far, the approaches described do not detect correspondences with any semantic meaning; in other words, the returned points cannot truly be `named' and there is no guarantee the same points (or even the same number of points) will be identified in different test images. Instead, this section will focus on techniques which predict a set of keypoint locations which are specified in a pre-defined list (for example: nose, tail tip, toe). In general, data-driven machine learning algorithms are used in order to learn an association between image appearance information and semantic keypoint labels. The techniques fall into two general categories: the former set of \emph{supervised techniques} rely on large image datasets manually annotated with keypoint locations, and the latter set of \emph{unsupervised techniques} learn the association through other means. 

\subsubsection{Supervised techniques}

Early work in the supervised prediction of landmarks began through the refinement of object detection methods to predict fine-grained object part labels and eventually progressed to keypoint locations. Perhaps the earliest techniques in this category made use of face part annotations (referred to as fiducial points) to align target faces to improve the face recongition accuracy. Human detection and pose estimation methods progressed from simple bounding box representations~\cite{hog}, to object part prediction~\cite{xxx,xxx}, poselets~\cite{pose-kposelets} and subsequently 2D keypoint localization~\cite{xxx,xxx}. Most commonly, methods aim to predict the location of important 2D human joints (such as the shoulders and wrists) in order to roughly approximate the subject's skeletal pose. For this reason, this task is commonly referred to as \emph{2D human pose estimation}. The earliest techniques represented humans as a graph of parts~\cite{human-rep-parts} and fit shape primitives (e.g. cylinders~\cite{pose-hogg}) to detected edges. Tree-based graphical models known as pictorial structures~\cite{pictorial-structures} were adopted and later made efficient~\cite{pose-felzen}. Improvements were made with models capable of expressing complex relationships betwen joints, such as flexible part mixtures~\cite{yang2013articulated,pose-johnson-mixtureparts}.

Before the popularization of modern deep learning architectures, various methods made use of features computed underneath predicted 2D landmark locations for fine-grained image classification tasks. For this reason, there are limited examples of keypoint datasets for animal categories such as dogs~\cite{liu2012dog} and birds~\cite{WelinderEtal2010}. Chapter 4 of this thesis will discuss StanfordExtra, a new dataset complete with annotated keypoint locations and segmentation masks for 12,000 dog images, encompassing 120 different breeds. At the time of publication, StanfordExtra is the largest annotated animal dataset of its kind. 

% https://arxiv.org/pdf/2012.13392.pdf
Recent works in 2D pose estimation typically employ convolutional neural networks (CNNs) due to the complex feature represenations that can be learnt for joints that, when applied discriminatively, enable accurate recongition. An early example~\cite{pose-embedding} learnt a pose embedding space with a CNN, and employed a nearest neighbour search algorithm to regress a pose. Later, deeper CNN models were used to regress facial point~\cite{pose-face-earlycnn} and full body~\cite{toshev2014deeppose} landmarks. More recent works improve robustness by regressing keypoint confidence maps~\cite{joint-training} rather than 2D keypoints directly, enabling spatial priors to be applied to remove outliers~\cite{cao2018openpose,Pfister15,Pfister14a,Charles16,joint-training,viewpoints-keypoints,pishchulin2016deepcut}. More recent methods are able to directly produce accurate confidence maps through a multi-stage pipeline~\cite{wei2016cpm}. Of particular note are hourglass~\cite{newell2016stacked} (relied upon in this thesis Chapter 3) and multi-level~\cite{sun2019deep,Xiao_2018_ECCV} structures, which combine global reasoning of full-body attributes and of fine-grained details. A related class of methods~\cite{guler2018densepose, taylor2012vitruvian} focus on \emph{dense} human pose estimation, which relate all 2D image pixels to a representative 3D surface of the human body. 

Modern techniques in 2D human pose estimation demonstrate impressive accuracy on in-the-wild datasets, and deal with with parsing multiple subjects in challenging poses and in the presence of various occluders. However, part of what enables these achievements is the prevalance of large 2D keypoint datasets which can be used for training. Further discussion of available 2D keypoint datasets has been left for Chapter 5, in which they are considered in-depth. Further discussion on the history and advances in 2D human pose estimation are comprehensively reviewed in~\cite{2dpose-survey-1, 2dpose-survey-2}.

\subsubsection{Unsupervised learning}

As this thesis focuses on developing methods for animal reconstruction, it is useful to review techniques which operate without large 2D keypoint training datasets, which are scarce for animal subjects. Note that the methods in this section all describe approaches for determining point correspondences between different scenes. Under consideration are methods based on transfer learning, unsupervised learning and methods based on weak-supervision. 

Early correspondence techniques include dense alignment methods including SIFT-flow~\cite{siftflow} which employed optical flow methods to match image using SIFT features, and Bristow et al.~\cite{Bristow2015DenseSC} who demonstrate a method for learning per-pixel semantic correspondences using geometric priors. They also show examples on various animal categories. Recent unsupervised techniques learn \emph{category-specific} semantic priors by employing deep networks on large image collections.

Zhou et al.~\cite{flowweb-efros} demonstrate a method for solving correspondences across an image collection by enforcing cycle consistency. Kanazawa et al.~\cite{kanazawa2016warpnet} introduce WarpNet which predicts a dense 2D deformation field for bird images by learning from synthetic thin-plate spline warps generated on extracted silhouettes. Thewlis et al.~\cite{thewlis-unsup-sphere} apply a similar trick, by ensuring a consistent mapping of warped facial images to a spherical coordinate frame and show results on human and cats. Jakab et al.~\cite{unsup-articulated-objects} show they can estimate 2D human pose without training data data by leverging that between two frames of a simple video sequence, human body shape and texture remains reasonably similar but the pose (including global rotation) varies. They therefore construct an architecture that, given a pair of frames $(I, J)$ defines a network $f$ that given frame $I$ predicts a 2D location vector $y$. The system then combines this vector $y$ with the second frame $J$ and trains a secondary network $g$ to reconstruct the original frame $I$. Due to the limited capacity of $v$, the fact that apart from the pose, most of the information necessary for reconstruction is already available in $J$, the network eventually learns to encode 2D pose coordinates using $v$.

Transfer learning describes a family of methods in which a machine learning model is first \emph{pre-trained} to solve a related task (often making use of secondary dataset with may be larger in size) in order to accumulate knowledge which offers an advantage when solving the original task. DeepLabCut~\cite{mathis2018deeplabcut}, LEAP~\cite{leap-animal-pose} and DeepPoseKit~\cite{graving2019deepposekit} exemplify such techniques, in which existing architectures~\cite{pishchulin2016deepcut,newell2016stacked,densenet,mobilenetv2} are first trained to predict 2D human pose (making use of the large available datasets), and are then repurposed to predict 2D animal keypoints using few (generally 100s) training examples. Cao et al~\cite{animalpose} demonstrate a cross-domain adaptation technique, which transfers knowledge gained from a modestly-sized animal dataset to unseen animal types. There are also dense estimation techniques, which extend DensePose~\cite{guler2018densepose} described above to proximal animal classes~\cite{DenseposeEvo20}, such as chimpanzees, by aligning the geometry between the animal category to humans for which data is plentiful.

% Also under consideration are methods which learn from yet lesser sources of supervision. 

%https://www.robots.ox.ac.uk/~vedaldi/assets/pubs/jakab18unsupervised.pdf
%https://people.csail.mit.edu/celiu/SIFTflow/SIFTflow.pdf
%https://people.eecs.berkeley.edu/~tinghuiz/papers/cvpr15_flow.pdf
%https://www.robots.ox.ac.uk/~vedaldi/assets/pubs/thewlis18modelling.pdf
%https://www.robots.ox.ac.uk/~vedaldi/assets/pubs/thewlis17dense.pdf
%https://www.robots.ox.ac.uk/~vedaldi/assets/pubs/thewlis16fully-trainable.pdf

% https://reader.elsevier.com/reader/sd/pii/S0959438819301151?token=7A13D081FA0EE09BD23EDE5D517D499C7678CD59082C5B225CE01EC8063089BDC30D740FA31FFA6330F7FF6D2FEF2D89
% DeepLabCut: ResNet
% https://www.nature.com/articles/s41592-018-0234-5 (LEAP, StackedHourglass, recently sped up with MobileNet2)
% https://github.com/jgraving/DeepPoseKit
% https://elifesciences.org/articles/47994

% 3D DeepLabCut: https://www.nature.com/articles/s41596-019-0176-0
% AniPose: https://anipose.readthedocs.io/en/latest/
% DeepFly3D: https://elifesciences.org/articles/48571


% TODO: Find out a bit more about sun2019deep, Xiao 2018 etc.