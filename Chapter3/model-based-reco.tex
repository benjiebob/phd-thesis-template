\section{Methods for monocular reconstruction of articulated subjects}

    Having discussed methods for modelling articulated subjects, this section will discuss approaches for reconstructing the 3D shape and pose of a subject from a monocular image or video. It is important to note that this task is challenging and fundamentally ill-posed. In common with other challenging 3D reconstruction tasks, input images will typically exhibit variation in camera view, lighting and environmental occlusion. However, 3D reconstruction pipelines for articulated subjects must also deal with variation due to body shape, body pose, clothing and self-occlusion (body parts obscuring other parts). In addition, the challenge of reconstructing 3D models from 2D images is also inherently ambiguous. As explained by Toshev and Szegedy~\cite{toshev2014deeppose}, even if 2D structure can be determined (for example, using 2D keypoint prediction), the subsequent `lifting' step to recover 3D remains ill-posed, as the space of consistent 3D poses for given 2D landmark locations is infinite. It is for this reason that the history of monocular 3D reconstruction makes extensive use of 3D morphable models (or other geometric, temporal, structural priors), as they provide necessary optimization contraints.

    The theme of this section is therefore to discuss how 3D morphable models (3DMMs) can be incoporated into 3D reconstruction pipelines. In general, algorithms take as input an input image or video and predict a set of 3D model parameters $\alpha$ (often factored into shape $\shape$ and pose $\pose$). Once determined, the output parameters are then supplied to the morphable model's generator function $g: \alpha \mapsto \R{3}$ (e.g. $\SMPL: (\shape, \pose) \mapsto \R{3}$ or $\SMAL: (\shape, \pose) \mapsto \R{3}$) to produce vertex positions $V \subseteq \R{3}$ for the model $M = (V, T)$. Recall that in general the triangulation $T$ of such a model is fixed. For completeness and comparison, this section will make some mention of the small class of 3D reconstruction methods for articulated subjets which operate without an explicit 3D morphable model. Although currently a developing area, current work in this category typically requires either paired 3D training data, employ alternative (and arguably more restrictive) shape priors (e.g. symmetry constraints) or produce results of significantly lower fidelity.

    Methods which align a parametric 3D model to monocular input date back as far as 1963, in a seminal paper by Roberts~\cite{xxx}. Roberts presents a method which optimizes parameters for viewpoint and cuboidal shape primitives to reconstruct a 2D line image. Model-based methods have also been applied to understand object structure, starting with fitting of geometric primitives~\cite{xxx} and later with Active Shape Models~\cite{xxx} which learn deformation priors from a provided training set. Perhaps due to the numerous commercial applications, the majority of recent work in 3D shape and pose recovery focuses particuarly on \emph{humans} as a special case. The first example of such an approach is the seminal work of Blanz and Vetter~\cite{blanz-vetter} who built the first 3D morphable face model by aligning 3D scans and optimized the parameters to provide a fit to a single image. Since then, the research community has collected a multitude of open source human datasets which provide strong supervisory signals for training deep neural networks. These include include accurate 3D deformable template models~\cite{loper15smpl} generated from real human scans, 3D motion capture datasets~\cite{ionescu2013human3,vonmarcard2018recovering} and large 2D datasets~\cite{lin2014microsoft,johnson2010clustered,andriluka14cvpr} which provide keypoint and silhouette annotations. The combination of these publically available datasets and their incoporation into deep learning pipelines have led to impressive reconstruction results when tested on in-the-wild human images and videos. Unfortunately, the diversity among animal subjects and the practical challenges associated with data capture have resulted in few datasets being made available. Despite appearing superficially similar to human tracking, these factors result in specific challenges to animal tracking which must be carefully handled. Perhaps for this reason, the body of related literature for animal tracking is considerably sparser. The remainder of this section will focus on methods for 3D pose estimation, followed by 3D shape and pose reconstruction of human and animal bodies. Further discussion on techniques for body part (e.g. face, hands) reconstruction are deferred to the following survey papers~\cite{xxx, xxx}.

\subsection{3D Pose Estimation}

% The reconstruction of an arbitrary configuration of 3D points from a single monocular RGB image has three characteristics that affect its performance: (i) it is a severely ill-posed problem because similar image projections can be derived from different 3D poses; (ii) it is an ill-conditioned problem since minor errors in the loca- tions of the 2D body joints can have large consequences in the 3D space; and (iii) it suffers from high dimensionality ( Agarwal and Triggs, 2006 ).

    Techniques for 3D pose estimation output a set of 3D keypoint locations which can be combined to form a skeletal outline. Apart from basic limb measurements, no other shape detail (e.g. surface definition, object density etc.) is obtained. However, it should be noted that this output form is often perfectly satisfactory depending on the intended application. In particular, this family of techniques have found numerous applications in controllerless gaming (e.g. Microsoft Kinect~\cite{kinectpaper}), motion capture (e.g. for digital character generation~\cite{xxx}), gait analysis (e.g. identifying lameness in cattle~\cite{xxx}) and many more. 
    
    The general approach is to recover a 3D skeleton such that the 3D joints project to known or estimated 2D joints subject to anatomical priors. Early approaches in this category fit human stick figures with various constraints, including assumptions of fixed limb lengths~\cite{xxx}, length ratios~\cite{xxx} or that limb lengths are isometric across individuals and vary only in global scaling~\cite{xxx}. More advanced techniques built statistical models of shape variation using anthropometric tables or learnt them from motion capture data~\cite{barron2001estimating}.

    A broad category of appraoches for this are methods for \emph{non-rigid structure from motion}~\cite{xxx}. The general formulation is to express a 3D skeleton $S \in \RR{3}{P}$ on $P$ points as a linear combination of basis shapes $S_{1}, \dots, S_{k}$ where $S_{i} \in \RR{3}{P}$. Precisely:
    
    \begin{equation}
        S = \sum_{i=1}^K l_{i} \cdot S_{i} \quad S, S_{i} \in \RR{3}{P} \quad l_{i} \in \R{}
    \end{equation}
    
    Assuming scaled orthographic projection, the following expression represents the projection of $P$ points of $S$ into 2D image coordinates $(u_{i}, v_{i})$:
    
    \begin{equation}
        \begin{bmatrix}
            u_{1} & u_{2} & \dots & u_{P} \\
            v_{1} & v_{2} & \dots & v_{P} \\
        \end{bmatrix}
        = R \cdot ( \sum_{i=1}^{K} l_{i} \cdot S_{i} ) + T
    \end{equation}

    or equivalently:

    \begin{equation}
        \begin{bmatrix}
            u_{1} & u_{2} & \dots & u_{P} \\
            v_{1} & v_{2} & \dots & v_{P} \\
        \end{bmatrix}
        = 
        \begin{bmatrix}
            l_{1}R & \dots & l_{K}R
        \end{bmatrix}
        \cdot
        \begin{bmatrix}
            S_{1} \\
            S_{2} \\
            \dots \\
            S_{K} \\
        \end{bmatrix}
    \end{equation}

    \def\PT#1#2#3{#1_{#2}^{(#3)}}
    \def\T#1#2{#1^{(#2)}}

    This can then be extended to handle multiple views of the subject taken over a monocular video sequence. Let $(\PT{u}{i}{t}, \PT{v}{i}{t})$ denote the tracked 2D point at timestep $t$. This gives rise to the following system, taken over $N$ timesteps:

    \begin{equation}
        \underbrace{
        \begin{bmatrix}
            \PT{u}{i}{1} & \dots & \PT{u}{P}{1} \\
            \PT{v}{i}{1} & \dots & \PT{v}{P}{1} \\
            \PT{u}{i}{2} & \dots & \PT{u}{P}{2} \\
            \PT{v}{i}{2} & \dots & \PT{v}{P}{2} \\
            & \dots & \\
            \PT{u}{i}{N} & \dots & \PT{u}{P}{N} \\
            \PT{v}{i}{N} & \dots & \PT{v}{P}{N} \\
        \end{bmatrix}
        }_{W}
        = 
        \underbrace{
        \begin{bmatrix}
            \PT{l}{1}{1}\T{R}{1} & \dots & \PT{l}{K}{1}\T{R}{1} \\
            \PT{l}{1}{2}\T{R}{2} & \dots & \PT{l}{K}{2}\T{R}{2} \\
            & \dots & \\
            \PT{l}{1}{N}\T{R}{N} & \dots & \PT{l}{K}{2}\T{R}{N} \\
        \end{bmatrix}
        }_{Q}
        \cdot
        \underbrace{
        \begin{bmatrix}
            S_{1} \\
            S_{2} \\
            \dots \\
            S_{K}
        \end{bmatrix}
        }_{B}
    \end{equation}

    % https://arxiv.org/pdf/1705.03098.pdf

    This shows the tracking matrix $W$ can be factored into 2 matrices: $Q$ which contains the camera pose $\T{R}{t}$ and configuration weights $\PT{l}{1}{t}, \dots, \PT{l}{K}{t}$ per frame ${t}$. $B$ encodes the $K$ basis shapes $S_{i}$. This system can be factored with singular value decomposition to yield the shape basis $S_{i}$, per-frame camera rotations $R$ and per-frame configuration weights $l$. A number of techniques follow this formulation~\cite{xxx, xxx, xxx}, but start with a shape basis learnt from available motion capture datasets (e.g. CMU~\cite{xxx}).

    More recent approaches were designed to be fully automatic. Shotton et al.~\cite{kinectpaper} designed a commerically-available system for 3D human skeletal tracking which required a depth sensor. A generative 3D body model was used to synthesize a large training dataset of depth images with corresponding body part labels. Density estimators for each body part are then used in combination to localize body joints with a calculated confidence value. Taylor et al.~\cite{taylor2012vitruvian} predict dense correpsondences between image pixels (again, with depth so in $\R{3}$) and a representative 3D human body model, again by training on synthetic depth images. Chapter 3 of this thesis demonstrates a technique for predicting keypoints by training on synthetic \emph{silhouette} data, rendered from an animal deformable body model, which overcomes the need for depth imagery at test time.

    Automatic monocular approaches often take advantage of 2D keypoint or body part detectors when reasoning about 3D skeletons. Simo-Serra et al.~\cite{xxx, xxx} form a probabilistic model that models both 3D pose and 2D keypoints jointly, overcoming noise among 2D body parts. Other approaches~\cite{xxx, xxx} employ a two-stage pipeline; they begin by localizing 2D joint positions on an input image before running a subsequent optimization step that `lifts` these to a 3D pose. Tangential work~\cite{xxx} takes uses detected 2D joints to perform a nearest neighbour search in a 3D mocap dataset. The most recent two-stage pipelines rely on deep convolutional networks to predict keypoints. Examples of such systems include DeepPose~\cite{toshev2014deeppose}, an approach which employs a CNN to reason jointly about 2D landmark detection and 3D pose estimation from single RGB images. Pishchulin et al.~\cite{pishchulin2016deepcut} later introduced DeepCut which extends DeepPose to the multi-person case.

    State-of-the-art techniques now operate as a direct regression to a 3D pose. Most often, paired 3D training data (such as is available from datasets such as Human3.6M~\cite{xxx}) is required which is generally expensive to obtain, particularly for animal categories. One branch of approaches~\cite{tekin2016direct} predicts body configuration in terms of angles. Other approaches include Pavlakos et al.~\cite{xxx}, who use a 2D joint predictor~\cite{xxx} followed by a deep architecture to regress 3D heatmaps. Moreno-Noguer~\cite{xxx} learn a pairwise distance matrix from 2D-to-3D space in order to allow unlikely 3D predictions to be ruled out with a suitable prior. These techniques were designed under the assumption that neural networks would struggle to learn a `lifting' function from 2D to 3D pose. This assumption was corrected by Martinez et al.~\cite{xxx} who demonstrate the effectiveness of a simple architecture at regressing accurate 3D keypoints from 2D predictions. This technique was later interpreted probabilistically by The technique was interpreted probabilistically by Li et al.~\cite{xxx}, who handled ambiguity in the 2D-to-3D lifting problem with a mixture density network. Related work that predicts a depth segmentation (so not strictly 3D keypoints) is SURREAL~\cite{xxx} who train their network with data generated synthetically with a 3D human body model.


% \subsection{Toolkit}


\subsection{Model-based human shape and pose}

    % only discuss dense methods -- discuss methods that do/do not explicitly model shape

    This section will discuss methods for reconstructing a full 3D \emph{dense} human from a monocular image or video sequence. Early work in this category fit shape primitives combined into a kinematic tree to silhouettes extracted from the input~\cite{xxx, xxx, xxx}. The introduction of the 3D deformable human body model known as SCAPE~\cite{xxx} enabled various fitting approaches. Sigal et al~\cite{xxx} compute shape features from manually extracted silhouettes and use a mixture of experts formulation for predicting SCAPE model parameters. Later, Guan et al.~\cite{xxx} fit the SCAPE model to provided keypoints, extracted silhouettes, edges and shading cues. They also define an interpenetration term that penalizes self-intersecting body parts, although this does not lead to easy optimization. Hasler et al.~\cite{xxx}, Zhou et al.~\cite{xxx} and Chen et al.~\cite{xxx} present a similar approach, although show optimization only to input keypoints and manually or semi-manually (e.g. GraphCut~\cite{xxx}) extracted silhouettes.

    

    A significant advance was made by the introduction of SMPLify~\cite{xxx}, the first fully-automatic method for monocular 3D human pose and shape reconstruction. Many of the concepts presented by SMPLify are used throughout this thesis, making it worthy of study. 

    %     \begin{equation}
    %         E_{\text{data}}(\theta,U) =\sum_{i=1}^{n}s_{i} \cdot d(x_{i}, M(u_{i}; \theta))
    %     \end{equation}
    %     where $M(u_{i}, \theta)$ is the position of vertex $u_{i}$ on the vitruvian manifold mesh after having been displaced by an LBS deformation with respect to the pose~$\theta$. 

    %     The sheer quantity of correspondences greatly constrain their optimizer which works well, even on challenging input images. Much of this report focuses on how this paper can be extended to work for animal subjects, incorporating deep learning correspondence prediction and working from monocular RGB input data.


    \subsubsection{Fitting a 3D model to 2D keypoints}

    \def\J#1{J_\mathrm{#1}}

    SMPLify works by fitting the SMPL~\cite{loper15smpl} model to a set of 2D image locations predicted by DeepCut~\cite{xxx}, a deep convolutional neural network. For an input image $I$, DeepCut predicts a set of image keypoint locations $\J{est} \in \RR{23}{2}$ which correspond to locations on the 3D SMPL mesh $\J{\SMPL}$. Precisely $\J{\SMPL} = R_{\pose}(J(\shape))$ where $J(\shape)$ computes 3D skeleton positions from SMPL shape parameters $\shape$, and $R_{\pose}$ is the global rigid transformation effected by SMPL pose parameters $\pose$. A model fitting approach is then used to align the SMPL model to the predicted keypoint positions. This is achieved through optimizing the SMPL parameters $(\shape, \pose)$, global translation $\trans$ and camera parameters $K$, subject to priors over pose, shape and limb interpenetration priors. 

    The key energy term used in the optimization (and indeed throughout this thesis) is given by $\E{J}(\shape, \pose; K, \J{est})$ and measures the weighted 2D distance between estimated keypoints $\J{est}$ and the corresponding SMPL joints $J_{\SMPL}$.

    % E_{J}(\shape, \pose, \trans; K, \J{est}) = \sum_{joint, i} w_{i} \rho(\Pi_{K}(\J{\SMPL, i} - \J{est, i}))


    \begin{equation}
        E_{J}(\shape, \pose, \trans; K, \J{est}) = \sum_{\mathrm{joint}, i} w_{i} \rho(\Pi_{K}(\J{\SMPL, i} - \J{est, i}))
    \end{equation}

    The weighted 2D distance is implemented using the Geman-McClure~\cite{xxx} penalty function $\rho$ which helps deal with noisy DeepCut estimates. SMPLify implements $\Pi_{K}$ perspective camera model with known (or roughly initialized) focal length although others opt for orthographic projection. The following definition provides a quick primer for this:

    \begin{definition}[Primer on camera geometry]

        Perspective projection is a function which maps a 3D structure to blah blah.

        \begin{equation}
            2X = Y
        \end{equation}

        Orthographic projection is the following:

        \begin{equation}
            3X = Z
        \end{equation}

    \end{definition}

    The full energy formulation is then given as:

    \begin{equation}
        E(\shape, \pose) = E_{J}(\shape, \pose; K, J_{\text{est}}) + \lambda_{\pose}E_{\pose}(\pose) + \lambda_{\alpha}E_{\alpha}(\pose) + \lambda_{\text{sp}}E_{\text{sp}}(\pose; \shape) + \lambda_{\shape}E_{\shape}(\shape)
    \end{equation}

    where the following energy terms are employed, balanced according to the $\lambda$ scalar weights:

    \begin{itemize}
        \item $E_{\pose}(\pose)$ is referred to as a \textit{pose prior} which favours more likely poses by assigning large punishment to those that deviate from known poses collected from a large dataset.
        \item $E_{\shape}(\shape)$ is referred to as a \textit{shape prior} which favours more likely pose-invariant shape configurations by assigning large punishment to those that deviate from known shapes collected from a large dataset. 
        \item $E_{\alpha}(\pose)$ is a \textit{joint limit} prior which ensures particular joints remain within acceptable angle limits. For example, a knee joint in a human model should be prohibited from bending more than 5 degrees upwards.
        \item $E_{sp}(\pose; \shape)$ is an \textit{interpenetration} term, which can only be defined in such shape modelling approaches. Using both shape and pose from the model, it is possible to determine if any limbs are self-intersecting, or intersect other parts of the body and assign appropriate penalty.
    \end{itemize}

    % TODO: Variations on SMPLify
    SMPLify has recently undergone subsequent variations, including BLAH, BLAH, and an application to 3D quadruped reconstruction discussed later. Chapter 3 of this thesis will introduce a \emph{self-supervised} version of SMPLify that uses synthetic data for training, thereby overcoming the need for a large 2D dataset with manually-labelled keypoints.

    An example result can be seen in Figure \ref{fig:smplify}:

    \begin{figure}[H] % Example image
        \center{\includegraphics[width=0.95\linewidth]{fitting_smpl}}
        \caption{SMPLify: Fitting the SMPL model to the Leeds Sports Dataset.}
        \label{fig:smplify}
    \end{figure}
        
    \subsubsection{Direct regression}
    % TODO combine these paragraphs. There's too much Indirect learning here.
    The most recent, and state-of-the-art approaches employ deep learning techniques to solve the entire optimization problem by directly regressing shape and pose parameters of the template model. Tan et al.~\cite{tan17indirect} present a technique they term \emph{indirect learning} which learns an encoding $f: \RR{H}{W} \mapsto (\pose, \shape, \trans, K)$ of input images to SMPL pose and shape, translation and camera parameters. Their method makes use of a \emph{silhouette renderer} $R : (V, T) \mapsto \{0,1\}^{H \times W}$ (learnt using synthetic data) capable of producing a binary silhouette from a predicted SMPL mesh. In this way, $R$ allows the network's SMPL predictions to be supervised to ensure generated silhouettes match ground-truth annotations. Kanazawa et al~\cite{xxx} extended this work to with their Human Mesh Recovery~\cite{kanazawa18end} paper, making use of a learnt differentiable renderer and adding 2D/3D keypoint losses (similar to Eqt 99).

    % Insert network architecture diagram for this work. 
    
    % presented extended Perhaps a seminal paper in this category then followed, technique was then extended, the network can be constrained with supervisory losses to ensure predicted 3D SMPL bodies generates a silhouette learns a \emph{renderer}, which silhouettes from 3D SMPL bodies which enables a loss to be computed using silhouettes from In this work, an encoding of synthetic input images introduce an approach that directly regresses to SMPL parameters from synthetic images, ensuring suitable image jitters are applied to promote generality to real-world images. The method is termed Indirect Learning, and is trained from real human images with no known corresponding SMPL parameters. An autoencoder network is introduced, and a decoder first trained from synthetic (SMPL parameter, rendered image) pairs to construct an automatic renderer. This part of the network is then frozen, before the entire autoencoder is trained on many segmented human images that optimize the encoder to real-world examples. The end result is a process that is able to predict SMPL parameters from real-world human images. An example result is shown in Figure~\ref{fig:indirect_learning}.

    The abundance of available human data has supported the development of successful monocular 3D reconstruction pipelines~\cite{kolotouros19convolutional,kanazawa18end-to-end}. Such approaches rely on accurate 3D data to build detailed priors over the distribution of human shapes and poses, and use large 2D keypoints datasets to promote generalization to ``in-the-wild'' scenarios. Silhouette data has also been shown to assist in accurate reconstruction of clothes, hair and other appearance detail~\cite{pifuSHNMKL19,alldieck2019learning}.
    While the dominant paradigm in human reconstruction is now end-to-end deep learning methods, SPIN~\cite{kolotouros19learning} show impressive improvement by incorporating an energy minimization process within their training loop to further minimize a 2D reprojection loss subject to fixed pose \& shape priors. Inspired by this innovation, we learn an iteratively-improving shape prior by applying expectation maximization 
    during the training process.

    Of course, these techniques are typically data hungry, requiring not only 3D morphable models but also 3D supervision per training image and large 

    % \begin{figure}[H] % Example image
    %     \center{\includegraphics[width=0.75\linewidth]{indirect_learning}}
    %     \caption{Indirect learning method regressing to SMPL parameters from an RGB video sequence. Reprinted from~\cite{tan17indirect}.}
    %     \label{fig:indirect_learning}
    % \end{figure}

    \subsubsection{Model-free techniques}

    Talk about PiFu + animals + articulated Nerf.


\subsection{Model-based animals}

    We want to do model based reconstruction to impose a prior on the fitting and also automatically recover an interpretable fit.

    \input{Chapter3/lit-review-tbl.tex}

    Table~\ref{tab:literature} summarizes previous work on animal reconstruction.
    It is interesting to note that while several papers demonstrate reconstruction across species, which {\em prima facie} is a richer class than just dogs, the test-time requirements (e.g. manually-clicked keypoints/silhouette segmentations, input image quality etc.) are considerably higher for those systems.
    Thus we claim that the achievement of reconstructing a full range of dog breeds, 
    with variable fur length, varying shape and pose of ears, and with considerable occlusion, is a significant contribution.

    A major impediment to research in 3D animal reconstruction has been the lack of a strong evaluation benchmark, with most of the above methods showing only qualitative evaluations or providing quantitative results on fewer than 50 examples. To remedy this, we introduce \emph{StanfordExtra}, a new large-scale dataset which we hope will drive further progress in the field. 

    While animals are often featured in computer vision literature, there are still relatively few works that focus on accurate 3D animal reconstruction. 

    A primary reason for this is the lack of large scale 3D datasets stemming from the practical challenges associated with 3D motion capture, as well as a lack of 2D data which captures a wide variety of animals. The recent Animal Pose dataset~\cite{animalpose} is one such 2D alternative, but contains significantly fewer labelled images than our new StanfordDogs dataset (4,000 compared to 20,580 in ). 
    On the other hand, animal silhouette data is plentiful~\cite{lin2014microsoft,everingham2010pascal,DAVIS2017-2nd}.

    % \subsubsection{Animals intro}

    % Cashman and Fitzgibbon~\cite{cashman2013shape} obtained one of the first 3D morphable animal models, but their work was limited to small classes of objects (e.g. dolphins, pigeons), and did not incorporate a skeleton.  Their work also showed the use of the 2D silhouette for fitting, which is key to our method. 
    % Reinert {\em et al.} \cite{reinert2016animated} meanwhile construct 3D meshes by fitting generalized cylinders to hand-drawn skeletons.
    % Combined skeletal and morphable models were used by Khamis {\em et al.}~\cite{hand-shape} for modelling the human hand, and Loper {\em et al.}~\cite{loper15smpl} in the SMPL model which has been extensively used for human tracking. 

    % The SMPL model was extended to animals by Zuffi {\em et al.}~\cite{zuffi2017menagerie}, where the lack of motion capture data for animal subjects is cleverly overcome by building the model from $41$ 3D scans of toy figurines from five quadruped families in arbitrary poses. Their paper demonstrates single-frame fits of their model to real-world animal data, showing that despite the model being built from ``artists' impressions'' it remains an accurate model of real animals. This is borne out further by our work.  Their paper did however depend on per-frame human annotated keypoint labels, which would be costly and challenging to obtain for large video sequences. This work was recently extended~\cite{zuffi_lions} with a refinement step that optimizes over model vertex positions. This can be considered independent to the initial SMAL model fit and would be trivial to add to our method.

    \subsubsection{Learning animal shape from unrelated 2D images}
    Cashman and Fitzgibbon~\cite{cashman2013shape} introduce an optimization technique able to recover a parameterized, morphable 3D model from unrelated 2D images depicting examples of the target class. The method requires user-supplied 2D object outlines and point constraints for each image, and a single rigid mesh for the entire object class. The authors demonstrate recovering an 8-parameter morphable dolphin model from 32 images obtained from Google. To reduce required user activity, it is reasonable to assume that given sufficient labelled training data, it would be simple to manipulate a convolutional network architecture able to perform foreground / background segmentation and identify human key points (say, joints) for the desired object class. The system achieves impressive results when optimizing over both pose and shape parameters across a range of object classes, but struggles for articulated models such as polar bears.

    \begin{figure}[H] % Example image
        \center{\includegraphics[width=0.7\linewidth]{dolphins}}
        \caption{8-parameter dolphin model with annotated contour (left) and contour generators (middle and right).}
        \label{fig:cashman_fitzgibbon}
    \end{figure}


    \subsubsection{Fitting to animal video sequences (Stebbing)}

    Stebbing et al.~\cite{arap_stebbing} introduce a technique capable of fitting a template mesh to live video sequences for a range of different animal species. Some user interaction is required in order to segment the animal from the background and to provide sparse 3D-mesh-to-2D-image key point correspondences. This work only operates on input video sequences (rather than single frames), so a number of temporal terms are incorporated that encourage sensible inter-frame model deviation. The system requires an annotated input template mesh representative of the target animal species. Note that this work does not require the template mesh to have an inner skeletal structure. However, the user assists an ARAP-style term by assigning each mesh vertex $v_i$ to one of $M$ groups which share a set of basis rotations $B_{m}$. 

    \begin{figure}[H]
        \centering
        \begin{subfigure}{0.5\textwidth}
        \centering
            \includegraphics[height=0.5\linewidth]{arapsfm/arap_annotated_template}
            \caption{Template mesh with joint movement constraints.}
        \end{subfigure}%
        \begin{subfigure}{0.5\textwidth}
        \centering
            \includegraphics[height=0.5\linewidth]{arapsfm/arap_point_tracks}
            \caption{Example of user supplied point tracks.}
        \end{subfigure}%
        \caption{User input required for the deformable mesh animation algorithm, reprinted from~\cite{arap_stebbing}.}
        \label{fig:arap_user}
    \end{figure} 

    Through reasonably accurate pose fitting and by allowing some pose-invariant shape deformation, this work produces smooth meshes which are often a good match to the input video. Moreover, their experimentation shows that ARAP is a useful prior for reconstructing articulated, non-rigid motion in instances that an internal skeleton is a priori unknown. However, the shape attributes for the reconstructed model are not particularly accurate, which results in frequent errors appearing at internal occluding contours. In addition, the large non-convex optimization algorithm is an expensive operation, taking around 1 minute per video frame on a standard Linux workstation.

    Results showing this work fitting a crude dog template mesh to a sample video obtained from YouTube are shown previously in Figure \ref{fig:intro_arap_output}. Figure \ref{fig:arap_output} shows another example, which operates on a template impala mesh.

    \begin{figure}[H]
        \center{\includegraphics[width=0.95\linewidth]{arapsfm/arap_impala}}
        \caption{Example of an impala template being fit to input video sequence, reprinted from~\cite{arap_stebbing}}
        \label{fig:arap_output}
    \end{figure}


    \subsubsection{Fitting the SMAL mesh to animal images}
    % Draw out the silhouette term as important here
    % Talk about subsequent SMALR + SMALST in the same section
    The SMAL paper briefly discusses a modification to the SMPLify approach in order to fit the SMAL model to RGB animal input images. The terms are largely the same, although the interpenetration term is omitted and joint positions are provided manually, rather than being predicted by a CNN. Finally, the optimizer requires a pre-segmented (i.e.\ silhouette) image which is also supplied by a user. An approach discussed in Chapter 4 builds on this work, so an in-depth description of this method is ommited here. However, an example result showing the result of the optimizer fitting the SMAL mesh to an RGB image of a fox can be seen in Figure \ref{fig:smalify}. Note that the whole optimization process takes around 1 minute per frame.

    Zuffi et al.~\cite{zuffi2017menagerie} made a significant contribution to 3D animal reconstruction research by releasing SMAL, a deformable 3D quadruped model (analagous to SMPL~\cite{loper15smpl} for human reconstruction) from $41$ scans of artist-designed toy figurines. The authors also released shape and pose priors generated from artist data. In this work we develop \emph{SMBLD}, an extension of SMAL that better represents the diverse dog category by adding scale parameters and refining the shape prior using our large image dataset.


    \begin{figure}[H] % Example image
        \center{\includegraphics[width=0.95\linewidth]{fitting_smal}}
        \caption{Fitting SMAL to a hand segmented animal, reprinted from~\cite{zuffi2017menagerie}.}
        \label{fig:smalify}
    \end{figure}


    \begin{definition}[Differentiable Rendering]
        The process of generating a 2D image from a 3D polygon mesh is known as rendering and can be achieved through a process known as raytracing. Raytracing is a rendering technique able to generate photorealistic 2D images from the scene. It can be considered the opposite process by which the human eye perceives the world, as this method involves lines being cast outwards, beginning at a point known as the \emph{camera origin}. Figure \ref{fig:raycasting} shows a typical set up, in which rays are cast from the camera origin through each pixel on the image plane. The colour for the pixel is obtained by following the ray through the scene until a light source or non-reflective surface is reached, taking into account any reflections or non-opaque scene items. Due to the considerable comptuation required, the operation is often parallelized and assigned to the GPU. However, the technique is typically considered unsuitable for real-time rendering of complex scenes (due to complex ray paths) or when high resolution images (many rays required) are needed. However, for this work, scenes are typically made up of a single non-reflective, solid mesh surface and contain no complex elements (e.g.\ shadows, non-constant lighting.

        \begin{figure}[H] % Example image
            \center{\includegraphics[width=0.5\linewidth]{ray_trace}}
            \caption{Diagram showing raycast rendering.~\cite{rendering}.}
            \label{fig:raycasting}
        \end{figure}

        It is also worth noting that the standard method for raycasting is not differentiable, causing problems for differentiable optimizers (including neural networks). However, alternative rendering methods~\cite{loper2014opendr} are available for these purposes.
    \end{definition}

    The SMAL authors~\cite{zuffi2017menagerie} demonstrate fitting their deformable 3D model to quadruped species using user-provided keypoint and silhouette dataset. SMALR~\cite{zuffi_lions} then demonstrated fitting to broader animal categories by incorporating multi-view constraints from video sequences. 3D-Safari~\cite{Zuffi19Safari} further improve by training a deep network on synthetic data (built using SMALR~\cite{zuffi_lions}) to recover detailed zebra shapes in the wild. Chapter 4 of this this will present a method that overcomes the need for hand-clicked keypoints by training a joint predictor on synthetic data. 

    A drawback of these approaches is their reliance on a test-time energy-based optimization procedure, which is susceptible to failure with poor quality keypoint/silhouette predictions and increases the computational burden. Chapter 5 of this method presents an automatic reconstruction method that overcomes the need for additional energy-based refinement, and is trained purely from single in-the-wild images.

    \subsubsection{Model-free techniques}

    While there have been various ``model-free'' approaches which do not rely on an initial template model to generate the 3D animal reconstruction, these techniques often do not produce a mesh~\cite{Agudo_2018_CVPR,novotny19c3dpo} or rely heavily on input 2D keypoints or video at test-time~\cite{vicente_3dv,Probst2018_ECCVa}. An exception is the end-to-end network of Kanazawa et al.~\cite{kanazawa2018birds}, although we argue that the bird category exhibits more limited articulation than our dog category.


% \subsubsection{Literature review tables}

% The closest work in terms of scale is the category-specific mesh reconstruction of Kanazawa et al.~\cite{kanazawa2018birds}, where 2850 images of birds were reconstructed.  However doing so for the complex pose and shape variations of dogs required the advances described in this paper.



