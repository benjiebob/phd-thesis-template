
\subsection{Learning a multi-modal shape prior.}

% Using a unimodal prior tends to result in predictions which look relatively similar in shape. To promote diversity among predicted 3D dog shapes, our method extends the formulation above to incorporate a mixture of Gaussians prior. We represent the mixture as a set of $M$ Gaussians, whose means are initialized by drawing samples from our existing prior:

% \begin{align}
%     \mu_{\shape}^{m} &\sim N(\mu_{\shape}, \Sigma_{\shape}) \\
%     \Sigma_{\shape}^{m} &:= \Sigma_{\shape}
% \end{align}

% We assign each training image $i$ with a set of mixture weights $\{w_{i}^{1}, \dots w_{i}^{M}\}$, where initially $w_{i}^{m} := \frac{1}{M}.$

% We can then apply the following mixture shape loss:

% \begin{equation}
%     L_{mixture}=\sum_{m=1}^M w_{i}^{m}L_{shape}(\shape_{i}, \mu_{\shape}^{m}, \Sigma_{\shape}^{m})
% \end{equation}

% In order to allow our mixture prior to learn ``in-the-loop" from the available training data, we apply expectation maximization every $k$ epochs during training. This step recomputes the means and variances for each mixture component based on the observed shapes in the training set, and updates the per-image mixture weights:

% \begin{align}
%     \mu_{\shape}^{m} :=& \mathrm{E}_{i}[\beta_{i}W_{i}^{m}]\\
%     \Sigma_{\shape}^{m} :=& \mathrm{Cov}_{i}[\beta_{i}W_{i}^{m}, \beta_{i}W_{i}^{m}]\\
%     w_{i}^{m} :=& \frac{L_{shape}(\shape_{i}, \mu_{\shape}^{m}, \Sigma_{\shape}^{m})}{\sum_{m'}^{M}L_{shape}(\shape_{i}, \mu_{\shape}^{m'}, \Sigma_{\shape}^{m'})}
% \end{align}


% \section{Attempt 2}

The previous section introduced a unimodal, multivariate Gaussian shape prior, based on mean $\mu_{\shape}$ and covariance matrix $\Sigma_{\shape}$. However, we find enforcing this prior throughout training tends to result in predictions which appear similar in 3D shape, even when tested on dog images of different breeds. We propose to improve diversity among predicted 3D dog shapes by extending the above formulation to a Mixture of $M$ Gaussians prior.  
The mixture shape loss is then given as:
\begin{align}
    \L{mixture}(\shape_{i}; \mu_{\shape}, \Sigma_{\shape}, \Pi_{\shape})
    % =&
    % \sum_{m=1}^M
    % \Pi_{\shape}^m
    % (\shape_{i} - \mu_{\shape}^{m})^{T} \inv{\Sigma_{\shape}^{m}} (\shape_{i} - \mu_{\shape}^{m})
    % \\
    =&
    \sum_{m=1}^M \Pi_{\shape}^{m}\L{shape}(\shape_{i}; \mu_{\shape}^{m}, \Sigma_{\shape}^{m})
\end{align}
Where $\mu_{\shape}^{m}$, $\Sigma_{\shape}^{m}$ and $\Pi_{\shape}^{m}$ 
are the mean, covariance and mixture weight respectively for Gaussian component 
$m$. For each component the mean is sampled from our existing unimodal prior and the covariance is set equal to the unimodal prior i.e. $\Sigma_{\shape}^{m} := \Sigma_{\shape}$. All mixture weights are initially set to $\frac{1}{M}$.

Each training image $i$ is assigned a set of latent variables $\{w_{i}^{1}, \dots w_{i}^{M}\}$ encoding the probability of the dog shape in image~$i$ being generated by component~$m$. 

\subsection{Expectation Maximization in the loop}

As previously discussed, our initial shape prior is obtained from artist data which we find is unrepresentative of the diverse shapes present in our real dog dataset. We address this by proposing to recover the latent variables $w_{i}^{m}$ and parameters ($\mu_{\shape}^{m}$, $\Sigma_{\shape}^{m}$ and $\Pi_{\shape}^{m}$) of our 3D shape prior by learning from monocular images of in-the-wild dogs and their 2D training labels in our training dataset.

We achieve this using Expectation Maximization (EM), which regularly updates the means and variances for each mixture component and per-image mixture weights based on the observed shapes in the training set. While training our 3D reconstruction network, we progressively update our shape mixture model with an alternating `E' step and `M' step described below:

\subsubsection{The `E' Step.}
The `E' step computes the expected value of the latent variables~$w_{i}^{m}$ 
assuming fixed $(\mu_{\shape}^{m}, \Sigma_{\shape}^{m}, \Pi_{\shape}^{m})$ for all $i \in \{1,\dots,N\}, m \in \{1,\dots,M\}$.

The update equation for an image $i$ with latest shape prediction $\shape_{i}$ 
and cluster $m$ with parameters $(\mu_{\shape}^{m}, \Sigma_{\shape}^{m}, \Pi_{\shape}^{m})$ 
is given as:
% distance between the latest shape prediction $\shape_{i}$ and the cluster $(\mu_{\shape}^{m}, \Sigma_{\shape}^{m})$

% \begin{align}
%     w_{i}^{m} 
%     :=& 
%     \frac{
%         (\shape_{i} - \mu_{\shape}^{m})^{T} \inv{\Sigma_{\shape}^{m}} (\shape_{i} - \mu_{\shape}^{m})
%     }
%     {
%         \sum_{m'}^{M}
%         (\shape_{i} - \mu_{\shape}^{m'})^{T} \inv{\Sigma_{\shape}^{m'}} (\shape_{i} - \mu_{\shape}^{m'})
%     }
%     \\
%     :=&
%     \frac{
%         L_{shape}(\shape_{i}, \mu_{\shape}^{m}, \Sigma_{\shape}^{m})
%     }
%     {
%         \sum_{m'}^{M}L_{shape}(\shape_{i}, \mu_{\shape}^{m'}, \Sigma_{\shape}^{m'})
%     } 
% \end{align}

\begin{align}
    w_{i}^{m} 
    :=& 
    \frac{
        \mathcal{N}(\shape_{i} | \mu_{\shape}^{m},\Sigma_{\shape}^{m})\Pi_{\shape}^{m}
    }
    {
        \sum_{m'}^{M}
        \mathcal{N}(\shape_{i} | \mu_{\shape}^{m'},\Sigma_{\shape}^{m'})\Pi_{\shape}^{m'}
    }
    % \\
    % :=&
    % \frac{
    %     L_{shape}(\shape_{i}, \mu_{\shape}^{m}, \Sigma_{\shape}^{m})
    % }
    % {
    %     \sum_{m'}^{M}L_{shape}(\shape_{i}, \mu_{\shape}^{m'}, \Sigma_{\shape}^{m'})
    % } 
\end{align}



\subsubsection{The `M' Step.}
The `M' step computes new values for $(\mu_{\shape}^{m}, \Sigma_{\shape}^{m}, \Pi_{\shape}^{m})$, assuming fixed $w_{i}^{m}$ for all $i \in \{1,\dots,N\}, m \in \{1,\dots,M\}$.

The update equations are given as follows:

% \begin{align}
%     \mu_{\shape}^{m} :=& 
%     \frac{
%         \sum_{i}w_{i}^{m}\shape_{i}
%     }
%     {
%         \sum_{i}w_{i}^{m}
%     }
%     \\
%     \Sigma_{\shape}^{m} :=& 
%     \frac{
%         \sum_{i}w_{i}^{m}
%         (\shape_{i} - \Sigma_{\shape}^{m})
%         (\shape_{i} - \Sigma_{\shape}^{m})^{T}
%     }
%     {
%         \sum_{i}w_{i}^{m}
%     }
%     \\
%     \Pi_{\shape}^{m} :=& 
%     \frac{1}{N}\sum_{i}{w_{i}^{m}}.
% \end{align}


\begin{equation}
    \mu_{\shape}^{m} := 
    \frac{
        \sum_{i}w_{i}^{m}\shape_{i}
    }
    {
        \sum_{i}w_{i}^{m}
    }
    \quad
    \Sigma_{\shape}^{m} :=
    \frac{
        \sum_{i}w_{i}^{m}
        (\shape_{i} - \Sigma_{\shape}^{m})
        (\shape_{i} - \Sigma_{\shape}^{m})^{T}
    }
    {
        \sum_{i}w_{i}^{m}
    }
    \quad
    \Pi_{\shape}^{m} :=
    \frac{1}{N}\sum_{i}{w_{i}^{m}}
\end{equation}
