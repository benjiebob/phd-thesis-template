
\section{Expectation Maximization in the Loop}

% Using a unimodal prior tends to result in predictions which look relatively similar in shape. To promote diversity among predicted 3D dog shapes, our method extends the formulation above to incorporate a mixture of Gaussians prior. We represent the mixture as a set of $M$ Gaussians, whose means are initialized by drawing samples from our existing prior:

% \begin{align}
%     \mu_{\shape}^{m} &\sim N(\mu_{\shape}, \Sigma_{\shape}) \\
%     \Sigma_{\shape}^{m} &:= \Sigma_{\shape}
% \end{align}

% We assign each training image $i$ with a set of mixture weights $\{w_{i}^{1}, \dots w_{i}^{M}\}$, where initially $w_{i}^{m} := \frac{1}{M}.$

% We can then apply the following mixture shape loss:

% \begin{equation}
%     L_{mixture}=\sum_{m=1}^M w_{i}^{m}L_{shape}(\shape_{i}, \mu_{\shape}^{m}, \Sigma_{\shape}^{m})
% \end{equation}

% In order to allow our mixture prior to learn ``in-the-loop" from the available training data, we apply expectation maximization every $k$ epochs during training. This step recomputes the means and variances for each mixture component based on the observed shapes in the training set, and updates the per-image mixture weights:

% \begin{align}
%     \mu_{\shape}^{m} :=& \mathrm{E}_{i}[\beta_{i}W_{i}^{m}]\\
%     \Sigma_{\shape}^{m} :=& \mathrm{Cov}_{i}[\beta_{i}W_{i}^{m}, \beta_{i}W_{i}^{m}]\\
%     w_{i}^{m} :=& \frac{L_{shape}(\shape_{i}, \mu_{\shape}^{m}, \Sigma_{\shape}^{m})}{\sum_{m'}^{M}L_{shape}(\shape_{i}, \mu_{\shape}^{m'}, \Sigma_{\shape}^{m'})}
% \end{align}


% \section{Attempt 2}

As previously discussed, the parameters for the mixture shape-scale prior are initialized using artist data which results in a prior that poorly represents the diverse shapes present in the real dog dataset. A key contribution presented in this chapter is the introduction of a procedure based on Expectation Maximization which gradually improves the representational power of the shape-scale prior by learning from monouclar images of in-the-wild dogs and their respective 2D training labels.

% \subsection{Assigning image weights}

\def\imgweight#1#2{w_{#1}^{#2}}

Firstly, each training image is assigned a set of latent variables $\{\imgweight{i}{1}, \dots, \imgweight{i}{M}\}$ which encode the likelihood that the dog in image~$i$ was generated according to each shape-scale component~$m \in \{1,\dots,M\}$. 

Expectation Maximization (EM) is then used to regularly update these image weights, as well as the parameters for the mixture $(\shapescalemu^{m},\shapescalecov^{m},\shapescalepi^{m})_{i=1}^{M}$. During the training loop of the 3D reconstruction network, these parameters are tuned according to alternating expectation (`E-') and maximization (`M-') steps which are described below:

% TODO - add me!
% Each training image $i$ is assigned a set of latent variables encoding the probability of the dog shape in image~$i$ being generated by component~$m$. 

% This is addressed by proposing to recover the latent variables $w_{i}^{m}$ and parameters $\shapescalemu^{m}$, $\shapescalecov^{m}$ and $\shapescalepi^{m}$ ($\mu_{\shape}^{m}$, $\Sigma_{\shape}^{m}$ and $\Pi_{\shape}^{m}$) of our 3D shape prior by learning from monocular images of in-the-wild dogs and their 2D training labels in our training dataset.

% We achieve this using Expectation Maximization (EM), which regularly updates the means and variances for each mixture component and per-image mixture weights based on the observed shapes in the training set. While training our 3D reconstruction network, we progressively update our shape mixture model with an alternating `E' step and `M' step described below:

\subsection{Expectation Maximization update steps}

\subsubsection{The `E' Step.}
The `E' step computes the expected value of the image weights~$w_{i}^{m}$ 
assuming fixed $(\shapescalemu^{m},\shapescalecov^{m},\shapescalepi^{m})$ for all $i \in \{1,\dots,N\}, m \in \{1,\dots,M\}$.

The update equation for an image $i$ with latest scale-shape prediction $\shapescale_{i}$ and cluster $m$ with parameters $(\shapescalemu^{m},\shapescalecov^{m},\shapescalepi^{m})$ 
is given as:

\begin{align}
    \imgweight{i}{m}
    :=& 
    \frac{
        \mathcal{N}(\shapescale_{i} | \shapescalemu^{m},\shapescalecov^{m})\shapescalepi^{m}
    }
    {
        \sum_{m'=1}^{M}
        \mathcal{N}(\shapescale_{i} | \shapescalemu^{m'},\shapescalecov^{m'})\shapescalepi^{m'}
    }
\end{align}

To improve numerical stability, this is formulated using the log-sum-exp trick:

\begin{align}
    \log{\imgweight{i}{m}}
    :=& 
    \frac{
        \log{
            \left[
            \shapescalepi^{m} (2\Pi)^{\frac{d}{2}}
            \left[\det{\shapescalecov^{m}}\right]^{-\frac{1}{2}}
            \exp{
                -\frac{1}{2}
                (\shapescale_{i} - \shapescalemu^{m})^{T}
                (\shapescalecov^{m})^{-1}
                (\shapescale_{i} - \shapescalemu^{m})
            }
        \right]
        }
    }
    {
        \log{
            \left[
            \sum_{m'=1}^{M}
            \shapescalepi^{m'} (2\Pi)^{\frac{d}{2}}
            \left[\det{\shapescalecov^{m'}}\right]^{-\frac{1}{2}}
            \exp{
                -\frac{1}{2}
                (\shapescale_{i} - \shapescalemu^{m'})^{T}
                (\shapescalecov^{m'})^{-1}
                (\shapescale_{i} - \shapescalemu^{m'})
            }
        \right]
        }
    }
\end{align}
And this simplifies to
\begin{multline}
    \log{\imgweight{i}{m}}
    :=
    \log{\shapescalepi^{m}} - \frac{1}{2}
        \left[
            d\log{2\pi} + \log{\det{\shapescalecov^{m}}} + M_{m}
        \right] \\ -
    \log{
        \sum_{m'=1}^{M} 
        \exp{
            \log{\shapescalepi^{m'}} - \frac{1}{2}
            \left[
                d\log{2\pi} + \log{\det{\shapescalecov^{m'}}} + M_{m'}
            \right]
        }
    }
\end{multline}
where
\begin{align}
M_{m} :=& 
    (\shapescale_{i} - \shapescalemu^{m})^{T}
    (\shapescalecov^{m})^{-1}
    (\shapescale_{i} - \shapescalemu^{m})    
\end{align}


\subsubsection{The `M' Step.}
The `M' step computes new values for $(\shapescalemu^{m},\shapescalecov^{m},\shapescalepi^{m})$, assuming fixed $\imgweight{i}{m}$ for all $i \in \{1,\dots,N\}$ and $m \in \{1,\dots,M\}$. The update equations are given as follows:

\begin{equation}
    \shapescalemu^{m} := 
    \frac{
        \sum_{i} \imgweight{i}{m}\shapescale_{i}
    }
    {
        \sum_{i} \imgweight{i}{m}
    }
    \quad
    \shapescalecov^{m} :=
    \frac{
        \sum_{i} 
        \imgweight{i}{m}
        (\shapescale_{i} - \shapescalecov^{m})
        (\shapescale_{i} - \shapescalecov^{m})^{T}
    }
    {
        \sum_{i}\imgweight{i}{m}
    }
    \quad
    \shapescalepi^{m} :=
    \frac{1}{N}\sum_{i} \imgweight{i}{m}
\end{equation}
