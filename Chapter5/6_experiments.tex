\section{Experiments}

% As with other methods, we observe that introducing a silhouette term into the network loss before the model's pose has been somewhat solved can result in unsatisfactory local minima. We overcome this by using a pre-training stage with the following loss terms:


In this section, the WLDO method is compared against competitive baselines. The protocol for evaluation is described first, followed by a quantitative and qualitative evaluation.

\subsection{Evaluation protocol}

Evaluation is based on the StanfordExtra dataset introduced earlier in the chapter, which contains 8,476 images for 120 breeds. These images are divided per-breed into an 80\%/20\% train and test split.

Two primary evaluation metrics are considered. IoU is the intersection-over-union of the projected model silhouette compared to the ground truth annotation and indicates the quality of the reconstructed 3D shape. Percentage of Correct Keypoints (PCK) computes the percentage of joints which are within a normalized distance (based on square root of 2D silhouette area) to the ground truth locations, and evaluates the quality of reconstructed 3D pose. In addition, PCK results are produced for various joint groups (legs, tail, ears, face) in order to compare the reconstruction accuracy for different parts of the dog. Also used for evaluation is a new comparison metric \emph{PCK-MAX}. This protocol is similar to the Percentage of Correct Keypoints (PCK) metric~\cite{yang2013articulated} but incorporates the `invisible' ground-truth points. The standard PCK metric ignores these points, meaning even correct 3D reconstructions will receive no credit. PCK-MAX instead assumes reconstructed 3D points for missing ground-truth data are correct, providing an interesting upper bound.

\subsection{Training procedure}

The WLDO network is trained in two stages. The first omits the silhouette loss which tends to lead the network to unsatisfactory local minima if applied too early. With the silhouette loss turned off, it is satisfactory to use the simple unimodal prior (and without EM) for this preliminary stage since there is no loss to specifically encourage a strong shape alignment. After this, the silhouette loss and mixture prior with $M=10$ clusters are introduced, and expectation maximization updates are applied every 50 epochs. The first stage is trained for 250 epochs and the second stage for 150. The entire training procedure takes approximately 96 hours on a single P100 GPU.

Recall that the training objective for our end-to-end system for predicting SMBLD parameters consistent with a monocular dog input image is given by:

\begin{equation}
    \L{opt}=\L{joints}+\L{sil}+\L{pose}+\L{shape}+\L{mixture}
\end{equation}

Each loss term is weighted with a scalar $\W{}$. The following details the specific weights used for each training stage:

\ss{Stage 1.} $\W{joints}=10.0,\W{pose}=1.0,\W{shape}=1.0,\W{sil}=0.0,\W{mixture}=0.0$. This stage is trained for 250 epochs using the Adam optimizer, with learning rate set to $10^{-4}$. 
\ss{Stage 2.} $\W{joints}=10.0,\W{pose}=0.5,\W{shape}=0.0,\W{sil}=100.0,\W{mixture}=0.1$. This stage is trained for 150 epochs and run the described EM update step every $K=50$ epochs. The number of clusters $M=10$ was selected based on a grid search over $M=1,5,10,25$ with IoU scores compared. The Adam optimizer is again applied with learning rate to $10^{-5}$.

To begin, WLDO is compared with various baseline methods. 3D Menagerie (3D-M)~\cite{zuffi2017menagerie} is an approach which fits the 3D SMAL model using per-image energy minimization. Creatures Great and SMAL (CGAS)~\cite{biggs2018creatures} is a three-stage method, which employs a joint predictor on silhouette renderings from synthetic 3D dogs, applies a genetic algorithm to clean predictions, and finally applies the SMAL optimizer to produce the 3D mesh.

At test-time both 3D-M and CGAS rely on manually-provided segementation masks, and 3D-M also relies on hand-clicked keypoints. In order to produce a fair comparison, we produce a set of \emph{predicted} keypoints for StanfordExtra by training the Stacked Hourglass Network~\cite{newell2016stacked} with 8 stacks and 1 block, and \emph{predicted} segmentation masks using DeepLab v3+~\cite{deeplabv3plus}. The Stacked Hourglass Network achieves 71.4\% PCK score, DeepLab v3+ achieves 83.4\% IoU score and the CGAS joint predictor achieves 41.8\% PCK score. 

%All methods are trained from scratch and evaluated on our Stanford Dog validation set.

% \input{eccv2020kit/tab_othernetworks}
% \input{tab_othernetworks}

Table~\ref{tab:baselinesfix}, \Cref{fig:comparison_1} and \Cref{fig:comparison_2} show the comparison against competitive methods. For full examination, results for 3D-M and CGAS are additionally provided in the scenario that ground-truth keypoints and/or segmentations are available at test time. 

The results show our end-to-end method outperforms the competitors when they are provided with predicted keypoints/segmentations (white rows). The WLDO method therefore achieves a new state-of-the-art on this 3D reconstruction task. In addition, the method is shown to achieve improved average IoU/PCK scores than competitive methods, even when competitors are provided ground truth annotations at test time (grey rows). Also demonstrated is the wider applicability of two contributions of this chapter (scale parameters and improved prior) in the improved performance of the 3D-M method when these are incorporated. Finally, WLDO's test-time speed is significantly faster than competitive methods as no subsequent energy minimization procedure is required. 

\input{Chapter5/Tables/tab_baselines_pckfix.tex}


\input{Chapter5/FigTex/fig_comparison_1.tex}
\input{Chapter5/FigTex/fig_comparison_2.tex}

%xxx: Note that CGAS does badly as it can't clean up using video
\subsection{Generalization to unseen dataset}

Table~\ref{tab:animalposefix} shows an experiment to compare how well our model generalizes to a new data domain. We test our model against the 3D-M~\cite{zuffi2017menagerie} method (using predicted keypoints and segmentations as above for fairness) on the recent Animal Pose dataset~\cite{animalpose}. The data preparation process is the same as for StanfordExtra and no fine-tuning was used for either method. Good results are achieved in this unseen domain and still improve over the 3D-M optimizer.

\input{Chapter5/Tables/tab_anipose_fix.tex}
\input{Chapter5/Tables/tab_anipose_abl_fix.tex}

\subsection{Ablation study}

Secondly, an ablation of the individual components of the WLDO method is provided in order to examine the effect of each contribution on the PCK/IoU performance. Three variants are evaluated: (1) \textbf{Ours w/o EM} that omits EM updates, (2) \textbf{Ours w/o MoG} which replaces the mixture shape prior with a unimodal prior, (3)~\textbf{Ours w/o Scale} which removes the scale parameters. 

The results in Table~\ref{tab:ablationfix} indicate that each individual component has a positive impact on the overall method performance. In particular, it can be seen that the inclusion of the EM and Mixture of Gaussians prior leads to an improvement in IoU, suggesting that the shape prior refinements steps help the model accurately fit the exact dog shape. Interestingly, adding the Mixture of Gaussians prior but omitting EM steps slightly hinders performance, most likely due to an sub-optimal initialization for the $M$ clusters. However, adding EM updates to the Mixture of Gaussian model improves all metrics except the ear keypoint accuracy. It is observed that the error here is caused by the shape prior learning slightly imprecise shapes for dogs with extremely ``floppy'' ears. Although there is good silhouette coverage for these regions, the fact the SMBLD model has only a single articulation point per ear causes a lack of flexibility that results in occasionally misplaced ear tips for these instances. This could be improved in future work by adding additional model joints to the ear. Finally, the increased model flexibility afforded by the SMBLD scale parameters is shown to improve the IoU/PCK scores. 

\subsection{PCK-MAX results}

The final evaluation metric analyses PCK-MAX, a protocol which provides an upper bound on the PCK score by assuming invisible keypoints were successfully recovered.

\input{Chapter5/Tables/tab_baselines_pckmax.tex}
\input{Chapter5/Tables/tab_anipose_pckmax.tex}
\input{Chapter5/Tables/tab_anipose_abl_pckmax.tex}


% \input{eccv2020kit/tab_ablation}


% We are able to use the dataset to examine which dog parts are the most challenging to position. 
% \input{eccv2020kit/fig_jointspreads}
% \anote{TODO: PCK tables and errors visualized on 3D dog.}
% \paragraph{Analysis over breeds}
% A significant benefit of our dog dataset is that the supplied breed labels allows for reconstruction performance to be evaluated over particular breeds. \anote{Figure} ranks the breeds by error.
% \input{eccv2020kit/tab_breed}

\subsection{Qualitative evaluation}

Figures \ref{fig:qualresults_se_1}, \ref{fig:qualresults_se_2} and \ref{fig:qualresults_se_3} shows a range of example system outputs when tested on range of StanfordExtra dogs with varying pose and shape and in challenging conditions. Figure \ref{fig:qualresults_ani} shows results on the Animal Pose dataset. Note that only StanfordExtra is used for training.


% \input{fig_comparison}

% \input{fig_qualresults}

% \input{fig_qual_results_animal_pose}
% \section{Failure Cases}

% \footnotetext[4]{PCK results in tables have been updated to match definitions of Yang and Ramanan~\cite{yang2013articulated} normalized by 2D silhouette area. Please see original tables and further details in the appendix.}
