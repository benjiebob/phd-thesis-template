%!TEX root = ../thesis.tex
%*******************************************************************************
%*********************************** First Chapter *****************************
%*******************************************************************************

\chapter{Introduction}  %Title of the First Chapter

\ifpdf
    \graphicspath{{Chapter1/Figs/Raster/}{Chapter1/Figs/PDF/}{Chapter1/Figs/}}
\else
    \graphicspath{{Chapter1/Figs/Vector/}{Chapter1/Figs/}}
\fi


%********************************** %First Section  **************************************
\section{Motivation} %Section - 1.1 

Animal welfare is an important concern for business and society, with an estimated 70 billion animals currently living under human care. Monitoring and
assessment of animal health can be assisted by obtaining accurate measurements
of an individual’s shape, volume and movement. These measurements should
be taken without interfering with the animal’s normal activity, and are needed
around the clock, under a variety of lighting and weather conditions, perhaps at
long range (e.g. in farm fields or wildlife parks). Therefore a very wide range of
cameras and imaging modalities must be handled. For small animals in captivity,
a depth camera might be possible, but techniques which can operate solely from
intensity data will have a much wider range of applicability

%********************************** %Second Section  *************************************
\section{Approach} %Section - 1.2

We address this problem using techniques from the recent human body and
hand tracking literature, combining machine learning and 3D model fitting. A
discriminative front-end uses a deep hourglass network to identify candidate 2D
joint positions. These joint positions are then linked into coherent skeletons by
solving an optimal joint assignment problem, and the resulting skeletons create
an initial estimate for a generative model-fitting back-end to yield detailed shape
and pose for each frame of the video.
Although superficially similar to human tracking, animal tracking (AT) has
some interesting differences that make it worthy of study:

Variability. In one sense, AT is simpler than human tracking as animals generally do not wear clothing. However, variations in surface texture are still
considerable between individuals, and the variety of shape across and within
species is considerably greater. If tracking is specialized to a particular species,
then shape variation is smaller, but training data is even harder to obtain.
Training data. For human tracking, hand labelled sequences of 2D segmentations and joint positions have been collected from a wide variety of sources [3–5].
Of these two classes of labelling, animal segmentation data is available in datasets
such as MSCOCO [4], PASCAL VOC [6] and DAVIS [7]. However this data is
considerably sparser than human data, and must be “shared” across species,
meaning the number of examples for a given animal shape class is considerably
fewer than is available for an equivalent variation in human shape. While segmentation data can be supplied by non-specialist human labellers, it is more
difficult to obtain joint position data. Some joints are easy to label, such as “tip
of snout”, but others such as the analogue of “right elbow” require training of
the operator to correctly identify across species.
Of more concern however, is 3D skeleton data. For humans, motion capture
(mocap) can be used to obtain long sequences of skeleton parameters (joint
positions and angles) from a wide variety of motions and activities. For animal
tracking, this is considerably harder: animals behave differently on treadmills
than in their quotidian environments, and although some animals such as horses
Creatures great and SMAL 3
and dogs have been coaxed into motion capture studios [8], it remains impractical
to consider mocap for a family of tigers at play.
These concerns are of course alleviated if we have access to synthetic training
data. Here, humans and animals share an advantage in the availability of parameterized 3D models of shape and pose. The recent publication of the Skinned
Multi-Animal Linear (SMAL) model [9] can generate a wide range of quadruped
species, although without surface texture maps. However, as with humans, it
remains difficult to generate RGB images which are sufficiently realistic to train
modern machine learning models. In the case of humans, this has been overcome
by generating depth maps, but this then requires a depth camera at test time [10].
The alternative, used in this work, is to generate 2D silhouette images so that
machine learning will predict joint heatmaps from silhouettes only

%********************************** % Third Section  *************************************
\section{Contributions}  %Section - 1.3 
In summary, the contributions of this thesis are as follows:
\begin{enumerate}
    \item We demonstrate a robust framework for 3D animal reconstruction using deformable template models
\end{enumerate}

\section{Co-Authored Papers}  %Section - 1.4

Extracts from this thesis appear in the following co-authored publications and preprints. Chapter 2 contains work from:

\begin{enumerate}
    \item Biggs Benjamin, Roddick Thomas
\end{enumerate}

\section{Thesis Structure}  %Section - 1.3 

The following four thesis chapters discuss important 
